{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning GPT-OSS-20B on AMD Strix Halo (Unsloth Benchmarks)\n",
        "\n",
        "**This notebook is an adapted 1-to-1 comparison of the standard Hugging Face finetuning pipeline, accelerated using [Unsloth](https://github.com/unslothai/unsloth).**\n",
        "\n",
        "### What is Unsloth?\n",
        "Unsloth speeds up LLM fine-tuning and reduces VRAM usage dramatically, allowing us to load 20B models natively in 4-bit.\n",
        "\n",
        "This notebook mirrors the exact configurations found in the standard HF notebook, using `unsloth.FastLanguageModel` as the backend engine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "source": [
        "## Setup and utilities\n",
        "\n",
        "We import PyTorch, Hugging Face Transformers, PEFT, and TRL.  \n",
        "All memory-tracking utilities are defined here for reuse across sections.  \n",
        "The model is loaded with `Mxfp4Config(dequantize=True)` to ensure stability during fine-tuning on ROCm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"UNSLOTH_SKIP_TORCHVISION_CHECK\"] = \"1\"\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from transformers import AutoTokenizer, Mxfp4Config\n\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "def reset_peak_mem():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "def report_peak_mem(tag: str = \"\"):\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Peak training memory{(' ' + tag) if tag else ''}: {torch.cuda.max_memory_allocated()/1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model selection and training parameters\n",
        "\n",
        "We fine-tune the **OpenAI GPT-OSS-20B (MXFP4)** model using bf16 dequantization for compatibility with Strix Halo.\n",
        "\n",
        "| Parameter | Meaning | Value |\n",
        "|:-----------|:---------|:------|\n",
        "| **MODEL** | Base pretrained model | `openai/gpt-oss-20b` |\n",
        "| **LR** | Learning rate | `2e-4` |\n",
        "| **EPOCHS** | Number of full dataset passes | `1` |\n",
        "| **BATCH_SIZE** | Samples per device per step | `4` |\n",
        "| **MAX_LEN** | Token limit per example | `2048` |\n",
        "\n",
        "Hyperparameters follow the ranges used in TRL\u2019s SFTTrainer examples and OpenAI\u2019s fine-tuning guide.  \n",
        "Longer sequences or higher batch sizes will increase memory quadratically under eager attention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL = \"openai/gpt-oss-20b\"\n",
        "model_name = MODEL.split(\"/\")[-1]\n",
        "\n",
        "LR = 2e-4\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 4\n",
        "MAX_LEN = 2048"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "quotes"
        ]
      },
      "source": [
        "## Section 1 \u2013 Non-reasoning SFT\n",
        "\n",
        "Dataset: [`Abirate/english_quotes`](https://huggingface.co/datasets/Abirate/english_quotes)  \n",
        "This dataset produces short, direct text completions without explicit reasoning or chain-of-thought.\n",
        "\n",
        "Purpose:\n",
        "- Validate the SFT training pipeline.  \n",
        "- Confirm LoRA integration on dequantized weights.  \n",
        "- Observe stable fine-tuning behavior before moving to reasoning datasets.\n",
        "\n",
        "Each section loads its own dataset to prevent memory overlap and to keep lifetime of large tensors scoped.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unsloth Advantage: Native 4-bit loading\n",
        "Instead of manually dequantizing `MXFP4` to `bf16` inside Strix Halo's VRAM\u2014which consumes enormous amounts of memory\u2014Unsloth's `FastLanguageModel` can load the model dynamically into optimized 4-bit.\n",
        "\n",
        "> **Note:** We are purposefully strictly keeping the same SFT parameters here to demonstrate 1-to-1 comparison against the baseline standard notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare the non-reasoning dataset locally to this section\n",
        "quotes_ds = load_dataset(\"Abirate/english_quotes\", split=\"train\").shuffle(seed=42).select(range(1000))\n",
        "\n",
        "def quotes_to_messages(ex):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": f\"Give me a quote about: {ex['tags']}\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"{ex['quote']} - {ex['author']}\"}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "quotes_ds = quotes_ds.map(quotes_to_messages, remove_columns=quotes_ds.column_names).train_test_split(test_size=0.2)\n",
        "print(f\"Quotes train: {len(quotes_ds['train'])}, test: {len(quotes_ds['test'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LoRA setup for non-reasoning SFT\n",
        "\n",
        "The model is loaded with bf16 dequantized weights (`Mxfp4Config(dequantize=True)`).  \n",
        "LoRA adapters are attached to selected MLP expert projections.  \n",
        "Only ~0.07 % of total parameters are trainable, drastically reducing memory footprint while maintaining adaptation ability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model and LoRA setup for non-reasoning SFT\n",
        "model_quotes, tokenizer_quotes = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL,\n",
        "    max_seq_length=MAX_LEN,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "lora_config_quotes = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=\"all-linear\",\n",
        "    target_parameters=[\n",
        "        \"7.mlp.experts.gate_up_proj\",\n",
        "        \"7.mlp.experts.down_proj\",\n",
        "        \"15.mlp.experts.gate_up_proj\",\n",
        "        \"15.mlp.experts.down_proj\",\n",
        "        \"23.mlp.experts.gate_up_proj\",\n",
        "        \"23.mlp.experts.down_proj\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_quotes = get_peft_model(model_quotes, lora_config_quotes)\n",
        "model_quotes.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on quotes\n",
        "args_quotes = SFTConfig(\n",
        "    output_dir=f\"out-unsloth-{model_name}-lora\",\n",
        "    max_length=MAX_LEN,\n",
        "    packing=False,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    warmup_ratio=0.03,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    learning_rate=LR,\n",
        "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
        "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=True,\n",
        "    report_to=\"none\",\n",
        "    save_safetensors=True,\n",
        "    save_total_limit=1\n",
        ")\n",
        "\n",
        "def quotes_formatting_func(example):\n",
        "    return [tokenizer_quotes.apply_chat_template(m, tokenize=False, add_generation_prompt=False) for m in example['messages']]\n",
        "\n",
        "trainer_quotes = SFTTrainer(\n",
        "    model=model_quotes,\n",
        "    args=args_quotes,\n",
        "    train_dataset=quotes_ds['train'],\n",
        "    eval_dataset=quotes_ds['test'],\n",
        "    formatting_func=quotes_formatting_func,\n",
        "    processing_class=tokenizer_quotes\n",
        ")\n",
        "\n",
        "reset_peak_mem()\n",
        "trainer_quotes.train()\n",
        "report_peak_mem(\"lora\")\n",
        "trainer_quotes.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup non-reasoning objects to free memory\n",
        "del model_quotes, trainer_quotes\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "reasoning"
        ]
      },
      "source": [
        "## Section 2 \u2013 Reasoning SFT\n",
        "\n",
        "Dataset: [`HuggingFaceH4/Multilingual-Thinking`](https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking)  \n",
        "This dataset provides structured *reasoning traces* \u2014 messages with explicit \u201cthinking\u201d content and final answers formatted in the **Harmony** schema.\n",
        "\n",
        "Goals:\n",
        "- Fine-tune GPT-OSS-20B to strengthen reasoning behavior and channel separation.  \n",
        "- Use separate LoRA adapters (`out-lora-reasoning`) to isolate reasoning behavior from standard instruction following.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the reasoning dataset only in this section\n",
        "reason_ds = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
        "print(f\"Reasoning samples: {len(reason_ds)}\")\n",
        "print(reason_ds[1][\"messages\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "reason_ds = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
        "\n",
        "# Filter only examples that actually include 'thinking'\n",
        "reasoning_examples = [\n",
        "    sample for sample in reason_ds\n",
        "    if any(m.get(\"thinking\") not in (None, \"\") for m in sample[\"messages\"])\n",
        "]\n",
        "\n",
        "print(f\"Total reasoning examples: {len(reasoning_examples)}\")\n",
        "print(reasoning_examples[0][\"messages\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Harmony chat format\n",
        "\n",
        "GPT-OSS models use the **Harmony** message template for chat and reasoning tasks.  \n",
        "Messages are encoded using tags such as:\n",
        "\n",
        "```\n",
        "<|start|>system<|message|>...<|end|>\n",
        "<|start|>user<|message|>...<|end|>\n",
        "<|start|>assistant<|channel|>analysis<|message|>...<|end|>\n",
        "<|start|>assistant<|channel|>final<|message|>...<|return|>\n",
        "\n",
        "```\n",
        "\n",
        "The tokenizer\u2019s `apply_chat_template()` method converts message lists into this structure automatically.  \n",
        "Harmony defines *channels* (`analysis`, `commentary`, `final`) that allow explicit reasoning steps before producing an answer.  \n",
        "Fine-tuning must preserve this formatting to keep reasoning and output generation aligned.\n",
        "\n",
        "ref: https://github.com/openai/harmony\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer_reason = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "sample = reasoning_examples[0]\n",
        "formatted = tokenizer_reason.apply_chat_template(\n",
        "    sample[\"messages\"],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=False\n",
        ")\n",
        "print(formatted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LoRA setup for reasoning SFT\n",
        "\n",
        "- We again dequantize MXFP4 weights to bf16 and attach adapters to the same MoE MLP projections.  \n",
        "- This ensures comparable capacity between the non-reasoning and reasoning adapters.  \n",
        "- Peak memory is higher here due to longer sequences and reasoning traces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model and LoRA setup for reasoning SFT\n",
        "model_reason, tokenizer_reason = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL,\n",
        "    max_seq_length=MAX_LEN,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "lora_config_reason = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=\"all-linear\",\n",
        "    target_parameters=[\n",
        "        \"7.mlp.experts.gate_up_proj\",\n",
        "        \"7.mlp.experts.down_proj\",\n",
        "        \"15.mlp.experts.gate_up_proj\",\n",
        "        \"15.mlp.experts.down_proj\",\n",
        "        \"23.mlp.experts.gate_up_proj\",\n",
        "        \"23.mlp.experts.down_proj\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_reason = get_peft_model(model_reason, lora_config_reason)\n",
        "model_reason.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on reasoning dataset\n",
        "args_reason = SFTConfig(\n",
        "    output_dir=f\"out-unsloth-{model_name}-reasoning-lora\",\n",
        "    max_length=MAX_LEN,\n",
        "    packing=False,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    warmup_ratio=0.03,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    learning_rate=LR,\n",
        "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
        "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=True,\n",
        "    report_to=\"none\",\n",
        "    save_safetensors=True,\n",
        "    save_total_limit=1\n",
        ")\n",
        "\n",
        "def reason_formatting_func(example):\n",
        "    return [tokenizer_reason.apply_chat_template(m, tokenize=False, add_generation_prompt=False) for m in example['messages']]\n",
        "\n",
        "trainer_reason = SFTTrainer(\n",
        "    model=model_reason,\n",
        "    args=args_reason,\n",
        "    train_dataset=reason_ds,\n",
        "    processing_class=tokenizer_reason,\n",
        "    formatting_func=reason_formatting_func,\n",
        ")\n",
        "\n",
        "reset_peak_mem()\n",
        "trainer_reason.train()\n",
        "report_peak_mem(\"reasoning-lora\")\n",
        "trainer_reason.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup reasoning objects to free memory\n",
        "del model_reason, trainer_reason\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference sanity check\n",
        "\n",
        "We load the merged reasoning adapter (`out-lora-reasoning`) into the base model for validation.  \n",
        "Generation runs with `attn_implementation=\"eager\"` and bf16 weights.  \n",
        "The prompt uses the Harmony chat template with an explicit reasoning language and user query to verify that the model:\n",
        "1. Generates coherent reasoning in the `analysis` channel.  \n",
        "2. Produces a correct, formatted final answer in the `final` channel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "base, tok = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL,\n",
        "    max_seq_length=MAX_LEN,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True\n",
        ")\n",
        "FastLanguageModel.for_inference(base)\n",
        "\n",
        "REASONING_LANGUAGE = \"German\"\n",
        "SYSTEM_PROMPT = f\"reasoning language: {REASONING_LANGUAGE}\"\n",
        "USER_PROMPT = \"\u00bfCu\u00e1l es el capital de Australia?\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": USER_PROMPT}\n",
        "]\n",
        "inputs = tok.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True, reasoning_effort=\"high\").to(merged.device)\n",
        "from transformers import TextStreamer\n_ = merged.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.6, streamer=TextStreamer(tok))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save and Export Options (Unsloth Native)\n",
        "Unsloth allows you to merge LoRA adapters back into the base model instantly and export to formats like GGUF or 16-bit without needing external scripts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA adapters to a 16-bit huggingface model\n",
        "# model_reason.save_pretrained_merged(\"gpt-unsloth-merged-16bit\", tokenizer_reason, save_method=\"merged_16bit\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export directly to GGUF format for llama.cpp/Ollama (Q8_0 for 8-bit, F16 for 16-bit)\n",
        "# model_reason.save_pretrained_gguf(\"gpt-unsloth-gguf\", tokenizer_reason, quantization_method=\"Q8_0\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}