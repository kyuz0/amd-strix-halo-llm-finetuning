{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning Gemma-3 on AMD Strix Halo (Unsloth Benchmarks)\n",
        "\n",
        "**This notebook is an adapted 1-to-1 comparison of the standard Hugging Face finetuning pipeline, accelerated using [Unsloth](https://github.com/unslothai/unsloth).**\n",
        "\n",
        "### What is Unsloth?\n",
        "Unsloth is a heavily optimized Open Source library that dramatically speeds up LLM fine-tuning (2x-5x faster) and reduces VRAM usage (up to 70% less) without degrading accuracy.\n",
        "\n",
        "**How does it work?**\n",
        "- It manually derives matrix differentials for backpropagation, skipping PyTorch's generic `.compile()` autograd overhead.\n",
        "- It writes standard operations like RoPE embeddings, Cross Entropy Loss, and MLP forward passes in highly optimized custom **Triton kernels**.\n",
        "- It recycles memory and dynamically quantizes weights on the fly.\n",
        "\n",
        "This notebook mirrors the exact configurations (Full Finetune, LoRA, 8-bit, QLoRA) found in the standard HF notebook, but uses `unsloth.FastLanguageModel` as the backend engine to demonstrate the speed and memory improvements on AMD hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"UNSLOTH_SKIP_TORCHVISION_CHECK\"] = \"1\"\n",
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from lib import full_cleanup\n",
        "\n",
        "def reset_peak_mem():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "def report_peak_mem(tag: str = \"\"):\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Peak training memory{(' ' + tag) if tag else ''}: {torch.cuda.max_memory_allocated()/1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Login into Hugging Face Hub\n",
        "#hf_token = '' \n",
        "#login(hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model selection and training parameters\n",
        "\n",
        "## What is Gemma?\n",
        "[Gemma](https://ai.google.dev/gemma) is Google\u2019s family of open, instruction-tuned large language models built for open research and efficient deployment.  \n",
        "It uses the same core architecture as Gemini models but is trained and released for reproducible downstream fine-tuning and evaluation.  \n",
        "Weights are available on Hugging Face under the `google/gemma-*` namespaces, in different sizes from **270M** to **27B** parameters.\n",
        "\n",
        "- `gemma-3-270m-it` \u2013 smallest, fast for experimentation.  \n",
        "- `gemma-3-1b-it` \u2013 light but expressive; good for quick SFT runs.  \n",
        "- `gemma-3-4b-it` \u2013 balanced for instruction-tuning tests.  \n",
        "- `gemma-3-12b-it` \u2013 large model, needs substantial memory.  \n",
        "- `gemma-3-27b-it` \u2013 very large; QLoRA recommended on Strix Halo.\n",
        "\n",
        "Reference: [Gemma model documentation](https://ai.google.dev/gemma/docs/overview)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#MODEL = \"google/gemma-3-270m-it\"\n",
        "MODEL = \"google/gemma-3-1b-it\"  # Default model\n",
        "#MODEL = \"google/gemma-3-4b-it\"\n",
        "#MODEL = \"google/gemma-3-12b-it\"\n",
        "#MODEL = \"google/gemma-3-27b-it\"\n",
        "\n",
        "#MODEL = \"Qwen/Qwen3-4B\"\n",
        "#MODEL = \"qwen/Qwen3-4B-Instruct-2507\"\n",
        "\n",
        "model_name = MODEL.split(\"/\")[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## About the training parameters\n",
        "The parameters below control the supervised fine-tuning (SFT) phase.\n",
        "\n",
        "| Parameter | Meaning | Typical Range / Source |\n",
        "|:-----------|:--------|:----------------------|\n",
        "| **LR** | Learning rate for optimizer (`adamw_*`) | `5e-5` to `1e-4` for small datasets; recommended baseline from Hugging Face TRL and Google Gemma guides. |\n",
        "| **EPOCHS** | Number of full passes over the dataset | 1\u20133 are typical for small SFT datasets. |\n",
        "| **BATCH_SIZE** | Samples per device per step | Adjust based on available VRAM / unified memory. |\n",
        "\n",
        "References:  \n",
        "- [Gemma fine-tuning examples](https://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune)  \n",
        "- [Hugging Face TRL SFTTrainer docs](https://huggingface.co/docs/trl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LR = 5e-5\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset\n",
        "We use a small dataset (`Abirate/english_quotes`) for demonstration.\n",
        "\n",
        "Replace it with your dataset containing `messages` for chat-format SFT tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = load_dataset(\"Abirate/english_quotes\", split=\"train\").shuffle(seed=42).select(range(1000))\n",
        "\n",
        "def format_chat(ex):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": f\"Give me a quote about: {ex['tags']}\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"{ex['quote']} - {ex['author']}\"}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "ds = ds.map(format_chat, remove_columns=ds.column_names)\n",
        "\n",
        "# Pre-format dataset for Unsloth SFTTrainer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer_for_data = AutoTokenizer.from_pretrained(MODEL)\n",
        "tokenizer_for_data = get_chat_template(tokenizer_for_data, chat_template=\"gemma-3\")\n",
        "\n",
        "def apply_template(examples):\n",
        "    texts = [tokenizer_for_data.apply_chat_template(m, tokenize=False, add_generation_prompt=False).removeprefix('<bos>') for m in examples[\"messages\"]]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "ds = ds.map(apply_template, batched=True)\n",
        "ds = ds.train_test_split(test_size=0.2)\n",
        "print(f\"Train: {len(ds['train'])}, Test: {len(ds['test'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds['train'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Full fine-tuning\n",
        "Full fine-tuning updates **all model parameters**, giving the highest adaptation quality and flexibility, but at the highest compute and memory cost.\n",
        "\n",
        "- Trains every weight with full gradients and optimizer states.  \n",
        "- Provides the best results when memory allows, but becomes infeasible for large models.  \n",
        "- Recommended for small and medium models only.  \n",
        "- Docs: [Gemma full fine-tune](https://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune)\n",
        "\n",
        "| Model | Peak Memory | Time (2 epochs) | Notes |\n",
        "|:------|-------------:|----------------:|:------|\n",
        "| Gemma-3-1B-IT | **19 GB** | ~2 min 52 s | Fits easily |\n",
        "| Gemma-3-4B-IT | **46 GB** | ~9 min | Manageable |\n",
        "| Gemma-3-12B-IT | **115 GB** | ~25 min | Pushes system limit |\n",
        "| Gemma-3-27B-IT | \u2014 | \u274c Out of memory | Not feasible |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unsloth Advantage: Full Finetuning\n",
        "Even without PEFT (LoRA), Unsloth accelerates full fine-tuning by up to 1.3x. It uses manually derived matrix differentials and optimized Triton kernels for operations like Cross Entropy Loss and RoPE embeddings, saving significant overhead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL,\n",
        "    max_seq_length = 512,\n",
        "    dtype = None,\n",
        "    load_in_4bit = False,\n",
        ")\n",
        "torch_dtype = model.dtype\n",
        "print(f\"Weights footprint: {model.get_memory_footprint()/1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = SFTConfig(\n",
        "    dataset_text_field=\"text\",\n",
        "    output_dir=f\"output-unsloth-{model_name}-full\",\n",
        "    max_length=512,\n",
        "    packing=False,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_checkpointing=False,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=LR,\n",
        "    fp16=True if torch_dtype == torch.float16 else False,\n",
        "    bf16=True if torch_dtype == torch.bfloat16 else False,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"none\",\n",
        "    dataset_kwargs={\"add_special_tokens\": False, \"append_concat_token\": True},\n",
        "    save_total_limit=1,  # keep only latest checkpoint\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(model=model, args=args, train_dataset=ds['train'], eval_dataset=ds['test'], processing_class=tokenizer)\n",
        "reset_peak_mem()\n",
        "trainer.train()\n",
        "report_peak_mem(\"full\")\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Free all memory\n",
        "full_cleanup(model, trainer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. LoRA (Low-Rank Adaptation)\n",
        "LoRA freezes the base model and inserts small low-rank adapter matrices into key projections, training only a tiny subset of weights.\n",
        "\n",
        "- Typically ~0.5\u20131.5 % of parameters are trainable.  \n",
        "- Big savings in memory and compute with minimal loss in quality.  \n",
        "- Excellent balance for mid-sized models.  \n",
        "- Docs: [PEFT LoRA](https://huggingface.co/docs/peft)\n",
        "\n",
        "| Model | Peak Memory | Time (2 epochs) | Notes |\n",
        "|:------|-------------:|----------------:|:------|\n",
        "| Gemma-3-1B-IT | **15 GB** | ~2 min | Very efficient |\n",
        "| Gemma-3-4B-IT | **30 GB** | ~5 min | Fast and stable |\n",
        "| Gemma-3-12B-IT | **67 GB** | ~13 min | Heavier but fits |\n",
        "| Gemma-3-27B-IT | \u2014 | \u274c Out of memory | Too large |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unsloth Advantage: 16-bit LoRA\n",
        "Unsloth excels at LoRA finetuning. By utilizing optimized custom kernels and an efficient `get_peft_model` implementation, it drastically speeds up PEFT training and reduces peak memory consumption by recycling memory efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL,\n",
        "    max_seq_length = 512,\n",
        "    dtype = None,\n",
        "    load_in_4bit = False,\n",
        ")\n",
        "torch_dtype = model.dtype\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model.print_trainable_parameters()\n",
        "print(f\"Weights footprint: {model.get_memory_footprint()/1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = SFTConfig(\n",
        "    dataset_text_field=\"text\",\n",
        "    output_dir=f\"output-unsloth-{model_name}-lora\",\n",
        "    max_length=512,\n",
        "    packing=False,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_checkpointing=False,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=LR,\n",
        "    fp16=True if torch_dtype == torch.float16 else False,\n",
        "    bf16=True if torch_dtype == torch.bfloat16 else False,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"none\",\n",
        "    dataset_kwargs={\"add_special_tokens\": False, \"append_concat_token\": True},\n",
        "    save_total_limit=1,  # keep only latest checkpoint\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(model=model, args=args, train_dataset=ds['train'], eval_dataset=ds['test'], processing_class=tokenizer)\n",
        "reset_peak_mem()\n",
        "trainer.train()\n",
        "report_peak_mem(\"lora\")\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Free all memory\n",
        "full_cleanup(model, trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 8-bit + LoRA\n",
        "This configuration loads the base model in **int8** with BitsAndBytes and trains LoRA adapters in fp16/bf16, reducing memory even further at some performance cost.\n",
        "\n",
        "- Roughly halves memory compared to plain LoRA.  \n",
        "- The warning  \n",
        "  `MatMul8bitLt: inputs will be cast from torch.float32 to torch.float16 during quantization`  \n",
        "  is expected \u2014 it adds minor casting overhead and can slow training slightly on ROCm.  \n",
        "- Docs: [BitsAndBytes integration](https://huggingface.co/blog/hf-bitsandbytes-integration)\n",
        "\n",
        "| Model | Peak Memory | Time (2 epochs) | Notes |\n",
        "|:------|-------------:|----------------:|:------|\n",
        "| Gemma-3-1B-IT | **13 GB** | ~8 min | Works well |\n",
        "| Gemma-3-4B-IT | **21 GB** | ~41 min | Stable |\n",
        "| Gemma-3-12B-IT | **43 GB** | ~2 h 38 min | Slow but fits |\n",
        "| Gemma-3-27B-IT | **32 GB** | \u2733 May fail near end | Memory-tight |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unsloth Advantage: 8-bit LoRA\n",
        "Instead of relying on the standard BitsAndBytes overhead and PyTorch `float16` casting on every forward pass, Unsloth's native `load_in_8bit=True` flag optimizes the underlying matrix multiplications, offering significant speedups over standard HF 8-bit configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL,\n",
        "    max_seq_length = 512,\n",
        "    dtype = None,\n",
        "    load_in_8bit = True,\n",
        "    load_in_4bit = False,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model.print_trainable_parameters()\n",
        "print(f\"Weights footprint: {model.get_memory_footprint()/1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = SFTConfig(\n",
        "    dataset_text_field=\"text\",\n",
        "    output_dir=f\"output-unsloth-{model_name}-8bit-lora\",\n",
        "    max_length=512,\n",
        "    packing=False,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    optim=\"adamw_8bit\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=LR,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"none\",\n",
        "    dataset_kwargs={\"add_special_tokens\": False, \"append_concat_token\": True},\n",
        "    save_total_limit=1,  # keep only latest checkpoint\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(model=model, args=args, train_dataset=ds['train'], eval_dataset=ds['test'], processing_class=tokenizer)\n",
        "reset_peak_mem()\n",
        "trainer.train()\n",
        "report_peak_mem(\"8bit-lora\")\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Free all memory\n",
        "full_cleanup(model, trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. QLoRA (4-bit NF4 + double quantization)\n",
        "QLoRA compresses the base weights to **4-bit NF4** with double quantization while training LoRA adapters in bf16 for numerical stability.  \n",
        "It offers the best trade-off between quality and memory use, allowing even large models to train.\n",
        "\n",
        "- Memory savings up to 4\u00d7 vs full precision; minimal quality loss.  \n",
        "- Best choice for large-scale fine-tuning under memory constraints.  \n",
        "- Docs: [Gemma QLoRA guide](https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora)\n",
        "\n",
        "| Model | Peak Memory | Time (2 epochs) | Notes |\n",
        "|:------|-------------:|----------------:|:------|\n",
        "| Gemma-3-1B-IT | **13 GB** | ~9 min | Efficient |\n",
        "| Gemma-3-4B-IT | **13 GB** | ~9 min | Similar footprint to 1B |\n",
        "| Gemma-3-12B-IT | **26 GB** | ~23 min | Fits comfortably |\n",
        "| Gemma-3-27B-IT | **19 GB** | \u2705 Runs successfully | Best option for 27B |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unsloth Advantage: 4-bit QLoRA\n",
        "This is Unsloth's primary focus. It utilizes \"Dynamic 4-bit Quantization\" and highly optimized Triton kernels to make 4-bit finetuning up to 2x faster than standard BitsAndBytes, while completely eliminating the 4-bit accuracy degradation often seen in standard transformers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL,\n",
        "    max_seq_length = 512,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model.print_trainable_parameters()\n",
        "print(f\"Weights footprint: {model.get_memory_footprint()/1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = SFTConfig(\n",
        "    dataset_text_field=\"text\",\n",
        "    output_dir=f\"output-unsloth-{model_name}-qlora\",\n",
        "    max_length=512,\n",
        "    packing=False,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"none\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    dataset_kwargs={\"add_special_tokens\": False, \"append_concat_token\": True},\n",
        "    save_total_limit=1,  # keep only latest checkpoint\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(model=model, args=args, train_dataset=ds['train'], eval_dataset=ds['test'], processing_class=tokenizer)\n",
        "reset_peak_mem()\n",
        "trainer.train()\n",
        "report_peak_mem(\"qlora\")\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Free all memory\n",
        "full_cleanup(model, trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Inference\n",
        "\n",
        "## Inference with a fully fine-tuned model\n",
        "This path loads the model checkpoint produced by **full fine-tuning** (`output-full`) and runs inference with **FlashAttention 2** enabled for efficiency.\n",
        "\n",
        "- The base model weights already include the fine-tuned parameters \u2014 no adapters to merge or attach.  \n",
        "- `attn_implementation=\"flash_attention_2\"` is safe for inference and improves throughput compared to eager mode.  \n",
        "- The Hugging Face `pipeline(\"text-generation\")` handles tokenization and sampling.  \n",
        "- Use this path when you trained with full fine-tuning or after you have merged adapters into the base model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Find all full finetune output directories\n",
        "full_checkpoints = sorted(glob.glob(\"output-unsloth-*full\"))\n",
        "\n",
        "if not full_checkpoints:\n",
        "    raise FileNotFoundError(\"No full fine-tuning directories found (looking for 'output-*full').\")\n",
        "\n",
        "print(f\"Found {len(full_checkpoints)} checkpoints:\")\n",
        "for i, ckpt in enumerate(full_checkpoints):\n",
        "    print(f\"[{i}] {ckpt}\")\n",
        "\n",
        "selection = input(f\"\\nSelect checkpoint index [0-{len(full_checkpoints)-1}] (default 0): \").strip()\n",
        "try:\n",
        "    idx = int(selection) if selection else 0\n",
        "    selected_path = full_checkpoints[idx]\n",
        "except (ValueError, IndexError):\n",
        "    print(f\"Invalid selection '{selection}', defaulting to {full_checkpoints[0]}\")\n",
        "    selected_path = full_checkpoints[0]\n",
        "\n",
        "print(f\"Loading model from: {selected_path}\")\n",
        "\n",
        "# Adapter load path\n",
        "from unsloth import FastLanguageModel\n",
        "base, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = selected_path if 'selected_path' in locals() else MODEL,\n",
        "    max_seq_length = 512,\n",
        ")\n",
        "FastLanguageModel.for_inference(base)\n",
        "tok = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import TextStreamer\nstreamer = TextStreamer(tok, skip_prompt=True)\npipe = pipeline(\"text-generation\", model=base, tokenizer=tok, streamer=streamer)\n",
        "\n",
        "sample = ds['test'][0]\n",
        "prompt = tok.apply_chat_template(sample[\"messages\"][:1], tokenize=False, add_generation_prompt=True)\n",
        "out = pipe(prompt, max_new_tokens=100, disable_compile=True)\n",
        "print(f\"User: {sample['messages'][0]['content']}\")\n",
        "print(f\"\\nExpected: {sample['messages'][1]['content']}\")\n",
        "print(\"\\nGenerated: \", end=\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds['test'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference with LoRA or QLoRA adapters\n",
        "This path loads the **original base model** and then attaches the **LoRA / QLoRA adapters** from the fine-tuned checkpoint (`output-lora`, `output-8bit-lora`, or `output-qlora`).\n",
        "\n",
        "- The base model remains frozen; the adapter layers modify its activations at runtime.  \n",
        "- You must load the base model first, then call `PeftModel.from_pretrained()` to apply the trained adapters.  \n",
        "- `attn_implementation=\"flash_attention_2\"` is again enabled for faster inference.  \n",
        "- Use this path when the model was trained with LoRA or QLoRA and you want to keep the adapters separate (for lightweight sharing or quick swapping).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Find all LoRA/QLoRA output directories (excluding full finetunes)\n",
        "# Matches anything containing 'lora' or 'qlora' in the name\n",
        "lora_checkpoints = sorted([d for d in glob.glob(\"output-unsloth-*\") if \"full\" not in d and (\"lora\" in d or \"qlora\" in d)])\n",
        "\n",
        "if not lora_checkpoints:\n",
        "    raise FileNotFoundError(\"No LoRA/QLoRA directories found (looking for 'output-*' excluding 'full').\")\n",
        "\n",
        "print(f\"Found {len(lora_checkpoints)} adapter checkpoints:\")\n",
        "for i, ckpt in enumerate(lora_checkpoints):\n",
        "    print(f\"[{i}] {ckpt}\")\n",
        "\n",
        "selection = input(f\"\\nSelect checkpoint index [0-{len(lora_checkpoints)-1}] (default 0): \").strip()\n",
        "try:\n",
        "    idx = int(selection) if selection else 0\n",
        "    selected_path = lora_checkpoints[idx]\n",
        "except (ValueError, IndexError):\n",
        "    print(f\"Invalid selection '{selection}', defaulting to {lora_checkpoints[0]}\")\n",
        "    selected_path = lora_checkpoints[0]\n",
        "\n",
        "print(f\"Loading base model: {MODEL}\")\n",
        "print(f\"Loading adapters from: {selected_path}\")\n",
        "\n",
        "# Adapter load path\n",
        "from unsloth import FastLanguageModel\n",
        "base, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = selected_path if 'selected_path' in locals() else MODEL,\n",
        "    max_seq_length = 512,\n",
        ")\n",
        "FastLanguageModel.for_inference(base)\n",
        "tok = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import TextStreamer\nstreamer = TextStreamer(tok, skip_prompt=True)\npipe = pipeline(\"text-generation\", model=base, tokenizer=tok, streamer=streamer)\n",
        "\n",
        "sample = ds['test'][0]\n",
        "prompt = tok.apply_chat_template(sample[\"messages\"][:1], tokenize=False, add_generation_prompt=True)\n",
        "out = pipe(prompt, max_new_tokens=100, disable_compile=True)\n",
        "print(f\"User: {sample['messages'][0]['content']}\")\n",
        "print(f\"\\nExpected: {sample['messages'][1]['content']}\")\n",
        "print(\"\\nGenerated: \", end=\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save and Export Options (Unsloth Native)\n",
        "Unsloth allows you to merge LoRA adapters back into the base model instantly and export to formats like GGUF or 16-bit without needing external scripts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA adapters to a 16-bit huggingface model\n",
        "# model.save_pretrained_merged(\"gemma-unsloth-merged-16bit\", tokenizer, save_method=\"merged_16bit\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export directly to GGUF format for llama.cpp/Ollama (Q8_0 for 8-bit, F16 for 16-bit)\n",
        "# model.save_pretrained_gguf(\"gemma-unsloth-gguf\", tokenizer, quantization_method=\"Q8_0\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}