{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-OSS-20B (MXFP4) on AMD Strix Halo (GFX1151)\n",
    "\n",
    "**Hardware**: AMD Strix Halo (GFX1151) – 128 GB unified memory.  \n",
    "**Objective**: demonstrate LoRA fine-tuning of *gpt-oss-20b* under MXFP4 quantization, comparing a non-reasoning dataset and a reasoning dataset, each isolated in its own memory scope and adapter.\n",
    "\n",
    "---\n",
    "\n",
    "### About GPT-OSS-20B\n",
    "`gpt-oss-20b` is an open-weight **20 billion-parameter mixture-of-experts (MoE)** model released by **OpenAI**.  \n",
    "Only a fraction of its experts (≈ 3.6 B active parameters per token) are used at inference time, which lowers compute while maintaining reasoning quality.  \n",
    "The model is distributed in **MXFP4 quantized format** to minimize storage and bandwidth.\n",
    "\n",
    "References:  \n",
    "- Model card: https://huggingface.co/openai/gpt-oss-20b  \n",
    "- Architecture overview: https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf\n",
    "\n",
    "---\n",
    "\n",
    "### Why MXFP4 and why dequantize on Strix Halo\n",
    "MXFP4 is a 4.25-bit floating-point format designed for OpenAI’s MoE weights.  \n",
    "On AMD ROCm (Strix Halo) hardware, **native MXFP4 kernels are not available**, so training must run on dequantized weights.\n",
    "\n",
    "Setting:\n",
    "```python\n",
    "Mxfp4Config(dequantize=True)\n",
    "````\n",
    "\n",
    "forces the model loader to expand MXFP4 tensors into **bf16**, ensuring stable gradients at the expense of higher memory use.\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook structure\n",
    "\n",
    "1. **Setup and utilities** – imports, helper functions, shared hyperparameters.\n",
    "2. **Non-reasoning SFT** – `Abirate/english_quotes` dataset (simple instruction/response).\n",
    "3. **Reasoning SFT** – `HuggingFaceH4/Multilingual-Thinking` dataset (Harmony reasoning traces).\n",
    "4. **Inference sanity check** – quick validation using the merged reasoning adapter.\n",
    "\n",
    "All training runs use `attn_implementation=\"eager\"` to avoid FlashAttention kernel incompatibilities on ROCm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "source": [
    "## Setup and utilities\n",
    "\n",
    "We import PyTorch, Hugging Face Transformers, PEFT, and TRL.  \n",
    "All memory-tracking utilities are defined here for reuse across sections.  \n",
    "The model is loaded with `Mxfp4Config(dequantize=True)` to ensure stability during fine-tuning on ROCm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Mxfp4Config\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "def reset_peak_mem():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def report_peak_mem(tag: str = \"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Peak training memory{(' ' + tag) if tag else ''}: {torch.cuda.max_memory_allocated()/1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and training parameters\n",
    "\n",
    "We fine-tune the **OpenAI GPT-OSS-20B (MXFP4)** model using bf16 dequantization for compatibility with Strix Halo.\n",
    "\n",
    "| Parameter | Meaning | Value |\n",
    "|:-----------|:---------|:------|\n",
    "| **MODEL** | Base pretrained model | `openai/gpt-oss-20b` |\n",
    "| **LR** | Learning rate | `2e-4` |\n",
    "| **EPOCHS** | Number of full dataset passes | `1` |\n",
    "| **BATCH_SIZE** | Samples per device per step | `4` |\n",
    "| **MAX_LEN** | Token limit per example | `2048` |\n",
    "\n",
    "Hyperparameters follow the ranges used in TRL’s SFTTrainer examples and OpenAI’s fine-tuning guide.  \n",
    "Longer sequences or higher batch sizes will increase memory quadratically under eager attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"openai/gpt-oss-20b\"\n",
    "model_name = MODEL.split(\"/\")[-1]\n",
    "\n",
    "LR = 2e-4\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "MAX_LEN = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "quotes"
    ]
   },
   "source": [
    "## Section 1 – Non-reasoning SFT\n",
    "\n",
    "Dataset: [`Abirate/english_quotes`](https://huggingface.co/datasets/Abirate/english_quotes)  \n",
    "This dataset produces short, direct text completions without explicit reasoning or chain-of-thought.\n",
    "\n",
    "Purpose:\n",
    "- Validate the SFT training pipeline.  \n",
    "- Confirm LoRA integration on dequantized weights.  \n",
    "- Observe stable fine-tuning behavior before moving to reasoning datasets.\n",
    "\n",
    "Each section loads its own dataset to prevent memory overlap and to keep lifetime of large tensors scoped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the non-reasoning dataset locally to this section\n",
    "quotes_ds = load_dataset(\"Abirate/english_quotes\", split=\"train\").shuffle(seed=42).select(range(1000))\n",
    "\n",
    "def quotes_to_messages(ex):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"Give me a quote about: {ex['tags']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"{ex['quote']} - {ex['author']}\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "quotes_ds = quotes_ds.map(quotes_to_messages, remove_columns=quotes_ds.column_names).train_test_split(test_size=0.2)\n",
    "print(f\"Quotes train: {len(quotes_ds['train'])}, test: {len(quotes_ds['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA setup for non-reasoning SFT\n",
    "\n",
    "The model is loaded with bf16 dequantized weights (`Mxfp4Config(dequantize=True)`).  \n",
    "LoRA adapters are attached to selected MLP expert projections.  \n",
    "Only ~0.07 % of total parameters are trainable, drastically reducing memory footprint while maintaining adaptation ability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and LoRA setup for non-reasoning SFT\n",
    "quant_quotes = Mxfp4Config(dequantize=True)\n",
    "model_quotes = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    "    use_cache=False,\n",
    "    quantization_config=quant_quotes,\n",
    ")\n",
    "tokenizer_quotes = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "lora_config_quotes = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=\"all-linear\",\n",
    "    target_parameters=[\n",
    "        \"7.mlp.experts.gate_up_proj\",\n",
    "        \"7.mlp.experts.down_proj\",\n",
    "        \"15.mlp.experts.gate_up_proj\",\n",
    "        \"15.mlp.experts.down_proj\",\n",
    "        \"23.mlp.experts.gate_up_proj\",\n",
    "        \"23.mlp.experts.down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_quotes = get_peft_model(model_quotes, lora_config_quotes)\n",
    "model_quotes.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on quotes\n",
    "args_quotes = SFTConfig(\n",
    "    output_dir=f\"out-{model_name}-lora\",\n",
    "    max_length=MAX_LEN,\n",
    "    packing=False,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    warmup_ratio=0.03,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    save_safetensors=True,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer_quotes = SFTTrainer(\n",
    "    model=model_quotes,\n",
    "    args=args_quotes,\n",
    "    train_dataset=quotes_ds['train'],\n",
    "    eval_dataset=quotes_ds['test'],\n",
    "    processing_class=tokenizer_quotes\n",
    ")\n",
    "\n",
    "reset_peak_mem()\n",
    "trainer_quotes.train()\n",
    "report_peak_mem(\"lora\")\n",
    "trainer_quotes.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup non-reasoning objects to free memory\n",
    "del model_quotes, trainer_quotes\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "reasoning"
    ]
   },
   "source": [
    "## Section 2 – Reasoning SFT\n",
    "\n",
    "Dataset: [`HuggingFaceH4/Multilingual-Thinking`](https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking)  \n",
    "This dataset provides structured *reasoning traces* — messages with explicit “thinking” content and final answers formatted in the **Harmony** schema.\n",
    "\n",
    "Goals:\n",
    "- Fine-tune GPT-OSS-20B to strengthen reasoning behavior and channel separation.  \n",
    "- Use separate LoRA adapters (`out-lora-reasoning`) to isolate reasoning behavior from standard instruction following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reasoning dataset only in this section\n",
    "reason_ds = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    "print(f\"Reasoning samples: {len(reason_ds)}\")\n",
    "print(reason_ds[1][\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "reason_ds = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    "\n",
    "# Filter only examples that actually include 'thinking'\n",
    "reasoning_examples = [\n",
    "    sample for sample in reason_ds\n",
    "    if any(m.get(\"thinking\") not in (None, \"\") for m in sample[\"messages\"])\n",
    "]\n",
    "\n",
    "print(f\"Total reasoning examples: {len(reasoning_examples)}\")\n",
    "print(reasoning_examples[0][\"messages\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmony chat format\n",
    "\n",
    "GPT-OSS models use the **Harmony** message template for chat and reasoning tasks.  \n",
    "Messages are encoded using tags such as:\n",
    "\n",
    "```\n",
    "<|start|>system<|message|>...<|end|>\n",
    "<|start|>user<|message|>...<|end|>\n",
    "<|start|>assistant<|channel|>analysis<|message|>...<|end|>\n",
    "<|start|>assistant<|channel|>final<|message|>...<|return|>\n",
    "\n",
    "```\n",
    "\n",
    "The tokenizer’s `apply_chat_template()` method converts message lists into this structure automatically.  \n",
    "Harmony defines *channels* (`analysis`, `commentary`, `final`) that allow explicit reasoning steps before producing an answer.  \n",
    "Fine-tuning must preserve this formatting to keep reasoning and output generation aligned.\n",
    "\n",
    "ref: https://github.com/openai/harmony\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_reason = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "sample = reasoning_examples[0]\n",
    "formatted = tokenizer_reason.apply_chat_template(\n",
    "    sample[\"messages\"],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "print(formatted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA setup for reasoning SFT\n",
    "\n",
    "- We again dequantize MXFP4 weights to bf16 and attach adapters to the same MoE MLP projections.  \n",
    "- This ensures comparable capacity between the non-reasoning and reasoning adapters.  \n",
    "- Peak memory is higher here due to longer sequences and reasoning traces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and LoRA setup for reasoning SFT\n",
    "quant_reason = Mxfp4Config(dequantize=True)\n",
    "model_reason = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    "    use_cache=False,\n",
    "    quantization_config=quant_reason\n",
    ")\n",
    "tokenizer_reason = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "lora_config_reason = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=\"all-linear\",\n",
    "    target_parameters=[\n",
    "        \"7.mlp.experts.gate_up_proj\",\n",
    "        \"7.mlp.experts.down_proj\",\n",
    "        \"15.mlp.experts.gate_up_proj\",\n",
    "        \"15.mlp.experts.down_proj\",\n",
    "        \"23.mlp.experts.gate_up_proj\",\n",
    "        \"23.mlp.experts.down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_reason = get_peft_model(model_reason, lora_config_reason)\n",
    "model_reason.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on reasoning dataset\n",
    "args_reason = SFTConfig(\n",
    "    output_dir=f\"out-{model_name}-reasoning-lora\",\n",
    "    max_length=MAX_LEN,\n",
    "    packing=False,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    warmup_ratio=0.03,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    save_safetensors=True,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer_reason = SFTTrainer(\n",
    "    model=model_reason,\n",
    "    args=args_reason,\n",
    "    train_dataset=reason_ds,\n",
    "    processing_class=tokenizer_reason\n",
    ")\n",
    "\n",
    "reset_peak_mem()\n",
    "trainer_reason.train()\n",
    "report_peak_mem(\"reasoning-lora\")\n",
    "trainer_reason.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup reasoning objects to free memory\n",
    "del model_reason, trainer_reason\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference sanity check\n",
    "\n",
    "We load the merged reasoning adapter (`out-lora-reasoning`) into the base model for validation.  \n",
    "Generation runs with `attn_implementation=\"eager\"` and bf16 weights.  \n",
    "The prompt uses the Harmony chat template with an explicit reasoning language and user query to verify that the model:\n",
    "1. Generates coherent reasoning in the `analysis` channel.  \n",
    "2. Produces a correct, formatted final answer in the `final` channel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "peft_reason = PeftModel.from_pretrained(base, \"out-gpt-oss-20b-lora-reasoning\")\n",
    "merged = peft_reason.merge_and_unload()\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "REASONING_LANGUAGE = \"German\"\n",
    "SYSTEM_PROMPT = f\"reasoning language: {REASONING_LANGUAGE}\"\n",
    "USER_PROMPT = \"¿Cuál es el capital de Australia?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "]\n",
    "input_ids = tok.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(merged.device)\n",
    "out = merged.generate(input_ids, max_new_tokens=256, do_sample=True, temperature=0.6)\n",
    "print(tok.batch_decode(out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
