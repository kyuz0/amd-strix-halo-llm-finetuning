{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Gemma-3 on AMD Strix Halo (GFX1151)\n",
    "\n",
    "**Hardware**: AMD Strix Halo (GFX1151) – 128 GB unified memory.  \n",
    "**Objective**: compare **Full fine-tuning**, **LoRA**, **8-bit + LoRA**, and **QLoRA** on Strix Halo, measuring runtime and peak memory.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods compared\n",
    "| Method | Description |\n",
    "|:--------|:-------------|\n",
    "| **Full fine-tuning** | Updates all model weights; maximum quality and flexibility, highest memory and compute cost. |\n",
    "| **LoRA** | Freezes the base model and trains small low-rank adapters on attention/MLP projections. |\n",
    "| **8-bit + LoRA** | Loads base weights in int8 using BitsAndBytes, trains LoRA adapters in fp16/bf16. |\n",
    "| **QLoRA (4-bit NF4 + double quant)** | Loads base weights in 4-bit NF4 with double quantization, trains LoRA adapters in bf16. |\n",
    "\n",
    "Official documentation:  \n",
    "- Full fine-tuning: https://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune  \n",
    "- QLoRA: https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora  \n",
    "- PEFT / LoRA: https://huggingface.co/docs/peft  \n",
    "- BitsAndBytes: https://huggingface.co/blog/hf-bitsandbytes-integration\n",
    "\n",
    "---\n",
    "\n",
    "### Empirical results on Strix Halo (2 epochs, `max_length = 512`)\n",
    "| Model | Full FT (GB / time) | LoRA (GB / time) | 8-bit + LoRA (GB / time) | QLoRA (GB / time) |\n",
    "|:------|:-------------------:|:----------------:|:------------------------:|:-----------------:|\n",
    "| **Gemma-3-1B-IT** | 19 / 2 m 52 s | 15 / 2 m | 13 / 8 m | 13 / 9 m |\n",
    "| **Gemma-3-4B-IT** | 46 / 9 m | 30 / 5 m | 21 / 41 m | 13 / 9 m |\n",
    "| **Gemma-3-12B-IT** | 115 / 25 m | 67 / 13 m | 43 / 2 h 38 m | 26 / 23 m |\n",
    "| **Gemma-3-27B-IT** | OOM | OOM | 32 / unstable | 19 / runs |\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset and sequence length\n",
    "- Demonstration dataset: [`Abirate/english_quotes`](https://huggingface.co/datasets/Abirate/english_quotes), 1 000 samples.  \n",
    "- `max_length = 512` defines the context window for tokenization.  \n",
    "  Longer sequences increase both compute and memory roughly **quadratically** under standard attention.  \n",
    "- During training, **`attn_implementation=\"eager\"`** is used, as recommended by Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use `attn_implementation=\"eager\"` for Gemma training\n",
    "- Gemma applies **attention logit soft-capping** which has known incompatibilities with FlashAttention. Google engineers note FA2 can require disabling soft-capping to work, which is not recommended during training quality-wise.  \n",
    "  Discussion: https://huggingface.co/google/gemma-2-9b-it/discussions/9\n",
    "- FA2 has shown **non-determinism** and training instabilities in some setups.  \n",
    "  Issue summary: https://github.com/huggingface/transformers/issues/31787  \n",
    "  Example training failures switching FA2→eager fixes: https://github.com/huggingface/transformers/issues/31997\n",
    "- On ROCm, FA kernels and backward paths are still evolving which can trigger NaNs or instability in training on some stacks.  \n",
    "  Example report: https://github.com/Dao-AILab/flash-attention/issues/1591\n",
    "\n",
    "Bottom line: keep **eager** for training Gemma. Use FA only for inference if your stack supports it reliably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from lib import full_cleanup\n",
    "\n",
    "def reset_peak_mem():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def report_peak_mem(tag: str = \"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Peak training memory{(' ' + tag) if tag else ''}: {torch.cuda.max_memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login into Hugging Face Hub\n",
    "#hf_token = '' \n",
    "#login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and training parameters\n",
    "\n",
    "## What is Gemma?\n",
    "[Gemma](https://ai.google.dev/gemma) is Google’s family of open, instruction-tuned large language models built for open research and efficient deployment.  \n",
    "It uses the same core architecture as Gemini models but is trained and released for reproducible downstream fine-tuning and evaluation.  \n",
    "Weights are available on Hugging Face under the `google/gemma-*` namespaces, in different sizes from **270M** to **27B** parameters.\n",
    "\n",
    "- `gemma-3-270m-it` – smallest, fast for experimentation.  \n",
    "- `gemma-3-1b-it` – light but expressive; good for quick SFT runs.  \n",
    "- `gemma-3-4b-it` – balanced for instruction-tuning tests.  \n",
    "- `gemma-3-12b-it` – large model, needs substantial memory.  \n",
    "- `gemma-3-27b-it` – very large; QLoRA recommended on Strix Halo.\n",
    "\n",
    "Reference: [Gemma model documentation](https://ai.google.dev/gemma/docs/overview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = \"google/gemma-3-270m-it\"\n",
    "MODEL = \"google/gemma-3-1b-it\"  # Default model\n",
    "#MODEL = \"google/gemma-3-4b-it\"\n",
    "#MODEL = \"google/gemma-3-12b-it\"\n",
    "#MODEL = \"google/gemma-3-27b-it\"\n",
    "\n",
    "#MODEL = \"Qwen/Qwen3-4B\"\n",
    "#MODEL = \"qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "model_name = MODEL.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the training parameters\n",
    "The parameters below control the supervised fine-tuning (SFT) phase.\n",
    "\n",
    "| Parameter | Meaning | Typical Range / Source |\n",
    "|:-----------|:--------|:----------------------|\n",
    "| **LR** | Learning rate for optimizer (`adamw_*`) | `5e-5` to `1e-4` for small datasets; recommended baseline from Hugging Face TRL and Google Gemma guides. |\n",
    "| **EPOCHS** | Number of full passes over the dataset | 1–3 are typical for small SFT datasets. |\n",
    "| **BATCH_SIZE** | Samples per device per step | Adjust based on available VRAM / unified memory. |\n",
    "\n",
    "References:  \n",
    "- [Gemma fine-tuning examples](https://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune)  \n",
    "- [Hugging Face TRL SFTTrainer docs](https://huggingface.co/docs/trl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5e-5\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "We use a small dataset (`Abirate/english_quotes`) for demonstration.\n",
    "\n",
    "Replace it with your dataset containing `messages` for chat-format SFT tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Abirate/english_quotes\", split=\"train\").shuffle(seed=42).select(range(1000))\n",
    "\n",
    "def format_chat(ex):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"Give me a quote about: {ex['tags']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"{ex['quote']} - {ex['author']}\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "ds = ds.map(format_chat, remove_columns=ds.column_names)\n",
    "ds = ds.train_test_split(test_size=0.2)\n",
    "print(f\"Train: {len(ds['train'])}, Test: {len(ds['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Full fine-tuning\n",
    "Full fine-tuning updates **all model parameters**, giving the highest adaptation quality and flexibility, but at the highest compute and memory cost.\n",
    "\n",
    "- Trains every weight with full gradients and optimizer states.  \n",
    "- Provides the best results when memory allows, but becomes infeasible for large models.  \n",
    "- Recommended for small and medium models only.  \n",
    "- Docs: [Gemma full fine-tune](https://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune)\n",
    "\n",
    "| Model | Peak Memory | Time (2 epochs) | Notes |\n",
    "|:------|-------------:|----------------:|:------|\n",
    "| Gemma-3-1B-IT | **19 GB** | ~2 min 52 s | Fits easily |\n",
    "| Gemma-3-4B-IT | **46 GB** | ~9 min | Manageable |\n",
    "| Gemma-3-12B-IT | **115 GB** | ~25 min | Pushes system limit |\n",
    "| Gemma-3-27B-IT | — | ❌ Out of memory | Not feasible |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL, dtype=\"auto\", device_map=\"auto\", attn_implementation=\"eager\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "torch_dtype = model.dtype\n",
    "print(f\"Weights footprint: {model.get_memory_footprint()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=f\"output-{model_name}-full\",\n",
    "    max_length=512,\n",
    "    packing=False,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    fp16=True if torch_dtype == torch.float16 else False,\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\",\n",
    "    dataset_kwargs={\"add_special_tokens\": False, \"append_concat_token\": True},\n",
    "    save_safetensors=True,\n",
    "    save_total_limit=1,  # keep only latest checkpoint\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(model=model, args=args, train_dataset=ds['train'], eval_dataset=ds['test'], processing_class=tokenizer)\n",
    "reset_peak_mem()\n",
    "trainer.train()\n",
    "report_peak_mem(\"full\")\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free all memory\n",
    "full_cleanup(model, trainer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LoRA (Low-Rank Adaptation)\n",
    "LoRA freezes the base model and inserts small low-rank adapter matrices into key projections, training only a tiny subset of weights.\n",
    "\n",
    "- Typically ~0.5–1.5 % of parameters are trainable.  \n",
    "- Big savings in memory and compute with minimal loss in quality.  \n",
    "- Excellent balance for mid-sized models.  \n",
    "- Docs: [PEFT LoRA](https://huggingface.co/docs/peft)\n",
    "\n",
    "| Model | Peak Memory | Time (2 epochs) | Notes |\n",
    "|:------|-------------:|----------------:|:------|\n",
    "| Gemma-3-1B-IT | **15 GB** | ~2 min | Very efficient |\n",
    "| Gemma-3-4B-IT | **30 GB** | ~5 min | Fast and stable |\n",
    "| Gemma-3-12B-IT | **67 GB** | ~13 min | Heavier but fits |\n",
    "| Gemma-3-27B-IT | — | ❌ Out of memory | Too large |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL, dtype=\"auto\", device_map=\"auto\", attn_implementation=\"eager\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "torch_dtype = model.dtype\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(f\"Weights footprint: {model.get_memory_footprint()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=f\"output-{model_name}-lora\",\n",
    "    max_length=512,\n",
    "    packing=False,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    fp16=True if torch_dtype == torch.float16 else False,\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\",\n",
    "    dataset_kwargs={\"add_special_tokens\": False, \"append_concat_token\": True},\n",
    "    save_safetensors=True,\n",
    "    save_total_limit=1,  # keep only latest checkpoint\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(model=model, args=args, train_dataset=ds['train'], eval_dataset=ds['test'], processing_class=tokenizer)\n",
    "reset_peak_mem()\n",
    "trainer.train()\n",
    "report_peak_mem(\"lora\")\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free all memory\n",
    "full_cleanup(model, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 8-bit + LoRA\n",
    "This configuration loads the base model in **int8** with BitsAndBytes and trains LoRA adapters in fp16/bf16, reducing memory even further at some performance cost.\n",
    "\n",
    "- Roughly halves memory compared to plain LoRA.  \n",
    "- The warning  \n",
    "  `MatMul8bitLt: inputs will be cast from torch.float32 to torch.float16 during quantization`  \n",
    "  is expected — it adds minor casting overhead and can slow training slightly on ROCm.  \n",
    "- Docs: [BitsAndBytes integration](https://huggingface.co/blog/hf-bitsandbytes-integration)\n",
    "\n",
    "| Model | Peak Memory | Time (2 epochs) | Notes |\n",
    "|:------|-------------:|----------------:|:------|\n",
    "| Gemma-3-1B-IT | **13 GB** | ~8 min | Works well |\n",
    "| Gemma-3-4B-IT | **21 GB** | ~41 min | Stable |\n",
    "| Gemma-3-12B-IT | **43 GB** | ~2 h 38 min | Slow but fits |\n",
    "| Gemma-3-27B-IT | **32 GB** | ✳ May fail near end | Memory-tight |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL, quantization_config=bnb_config, device_map=\"auto\", attn_implementation=\"eager\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(f\"Weights footprint: {model.get_memory_footprint()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=f\"output-{model_name}-8bit-lora\",\n",
    "    max_length=512,\n",
    "    packing=False,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\",\n",
    "    dataset_kwargs={\"add_special_tokens\": False, \"append_concat_token\": True},\n",
    "    save_safetensors=True,\n",
    "    save_total_limit=1,  # keep only latest checkpoint\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(model=model, args=args, train_dataset=ds['train'], eval_dataset=ds['test'], processing_class=tokenizer)\n",
    "reset_peak_mem()\n",
    "trainer.train()\n",
    "report_peak_mem(\"8bit-lora\")\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free all memory\n",
    "full_cleanup(model, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. QLoRA (4-bit NF4 + double quantization)\n",
    "QLoRA compresses the base weights to **4-bit NF4** with double quantization while training LoRA adapters in bf16 for numerical stability.  \n",
    "It offers the best trade-off between quality and memory use, allowing even large models to train.\n",
    "\n",
    "- Memory savings up to 4× vs full precision; minimal quality loss.  \n",
    "- Best choice for large-scale fine-tuning under memory constraints.  \n",
    "- Docs: [Gemma QLoRA guide](https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora)\n",
    "\n",
    "| Model | Peak Memory | Time (2 epochs) | Notes |\n",
    "|:------|-------------:|----------------:|:------|\n",
    "| Gemma-3-1B-IT | **13 GB** | ~9 min | Efficient |\n",
    "| Gemma-3-4B-IT | **13 GB** | ~9 min | Similar footprint to 1B |\n",
    "| Gemma-3-12B-IT | **26 GB** | ~23 min | Fits comfortably |\n",
    "| Gemma-3-27B-IT | **19 GB** | ✅ Runs successfully | Best option for 27B |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL, quantization_config=bnb_config, device_map=\"auto\", attn_implementation=\"eager\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(f\"Weights footprint: {model.get_memory_footprint()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=f\"output-{model_name}-qlora\",\n",
    "    max_length=512,\n",
    "    packing=False,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    dataset_kwargs={\"add_special_tokens\": False, \"append_concat_token\": True},\n",
    "    save_safetensors=True,\n",
    "    save_total_limit=1,  # keep only latest checkpoint\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(model=model, args=args, train_dataset=ds['train'], eval_dataset=ds['test'], processing_class=tokenizer)\n",
    "reset_peak_mem()\n",
    "trainer.train()\n",
    "report_peak_mem(\"qlora\")\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free all memory\n",
    "full_cleanup(model, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Inference\n",
    "\n",
    "## Inference with a fully fine-tuned model\n",
    "This path loads the model checkpoint produced by **full fine-tuning** (`output-full`) and runs inference with **FlashAttention 2** enabled for efficiency.\n",
    "\n",
    "- The base model weights already include the fine-tuned parameters — no adapters to merge or attach.  \n",
    "- `attn_implementation=\"flash_attention_2\"` is safe for inference and improves throughput compared to eager mode.  \n",
    "- The Hugging Face `pipeline(\"text-generation\")` handles tokenization and sampling.  \n",
    "- Use this path when you trained with full fine-tuning or after you have merged adapters into the base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter load path\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    f\"output-{model_name}-full\", dtype=\"auto\", device_map=\"auto\", attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=base, tokenizer=tok)\n",
    "\n",
    "sample = ds['test'][0]\n",
    "prompt = tok.apply_chat_template(sample[\"messages\"][:1], tokenize=False, add_generation_prompt=True)\n",
    "out = pipe(prompt, max_new_tokens=100, disable_compile=True)\n",
    "print(f\"User: {sample['messages'][0]['content']}\")\n",
    "print(f\"\\nExpected: {sample['messages'][1]['content']}\")\n",
    "print(f\"\\nGenerated: {out[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with LoRA or QLoRA adapters\n",
    "This path loads the **original base model** and then attaches the **LoRA / QLoRA adapters** from the fine-tuned checkpoint (`output-lora`, `output-8bit-lora`, or `output-qlora`).\n",
    "\n",
    "- The base model remains frozen; the adapter layers modify its activations at runtime.  \n",
    "- You must load the base model first, then call `PeftModel.from_pretrained()` to apply the trained adapters.  \n",
    "- `attn_implementation=\"flash_attention_2\"` is again enabled for faster inference.  \n",
    "- Use this path when the model was trained with LoRA or QLoRA and you want to keep the adapters separate (for lightweight sharing or quick swapping).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter load path\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL, dtype=\"auto\", device_map=\"auto\", attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "peft_model = PeftModel.from_pretrained(base, f\"output-{model_name}-lora\")\n",
    "\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=base, tokenizer=tok)\n",
    "\n",
    "sample = ds['test'][0]\n",
    "prompt = tok.apply_chat_template(sample[\"messages\"][:1], tokenize=False, add_generation_prompt=True)\n",
    "out = pipe(prompt, max_new_tokens=100, disable_compile=True)\n",
    "print(f\"User: {sample['messages'][0]['content']}\")\n",
    "print(f\"\\nExpected: {sample['messages'][1]['content']}\")\n",
    "print(f\"\\nGenerated: {out[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
